{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# # our groupings\n",
    "# seed_hashtags = pd.read_csv(\"community-grouping_20240809.csv\")\n",
    "# # all hashtags double hit\n",
    "# dat = pd.read_csv('double_hits_edges_no_dupes.csv')\n",
    "\n",
    "# open the secrets file\n",
    "with open('../data/secrets.json') as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "api_key = secrets['OPENAI_API_KEY_SR2']\n",
    "\n",
    "client = openai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rate limits\n",
    "def check_rate_limits(api_key):\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    try:\n",
    "        # Make a chat completion API call with raw response\n",
    "        api_response = client.chat.completions.with_raw_response.create(\n",
    "            model=\"gpt-4o\",  \n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Hello, this is a test message to check rate limits.\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Access the headers\n",
    "        headers = api_response.headers\n",
    "\n",
    "        # Print rate limit information\n",
    "        print(\"Rate Limit Information:\")\n",
    "        print(f\"Remaining Requests: {headers.get('x-ratelimit-remaining-requests')}\")\n",
    "        print(f\"Remaining Tokens: {headers.get('x-ratelimit-remaining-tokens')}\")\n",
    "        print(f\"Reset Tokens: {headers.get('x-ratelimit-reset-tokens')}\")\n",
    "        print(f\"Reset Requests: {headers.get('x-ratelimit-reset-requests')}\")\n",
    "\n",
    "        # Print the response content\n",
    "        response = api_response.parse()\n",
    "        print(f\"\\nResponse content: {response.choices[0].message.content}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Limit Information:\n",
      "Remaining Requests: 9999\n",
      "Remaining Tokens: 1999970\n",
      "Reset Tokens: 0s\n",
      "Reset Requests: 6ms\n",
      "\n",
      "Response content: Hello! Feel free to continue messaging if you have any questions or need information. I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "check_rate_limits(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IRR reliability with human annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted tags:\n",
      "['funny', 'love', 'depressionandanxiety', 'alcohol', 'weed', 'cognitiveenhancers', 'opioids', 'smoking', 'harmreductionsaveslives', 'vitamins', 'fyp', 'stoned', 'vaping', 'kensingtonphilly', 'medstudent', 'lgbtqia', 'addictionhumor', 'hope', 'addiction', 'beers', 'pot', 'modafinil', 'opiates', 'drinking', 'soberlife', 'creatine', 'greenscreen', 'high', 'nicotine', 'vancouverbc', 'nurselife', 'transgender', 'meme', 'struggle', 'opioidaddiction', 'alchohol', 'cannabis', 'nootropics', 'heroin', 'drink', 'druglaws', 'magnesium', 'tiktok', 'wasted', 'ecig', 'bayarea', 'exprostitute', 'lgbtqtiktok', 'kindness', 'substanceabuse', 'vodka', 'greens', 'nootrostatic', 'fent', 'shmokesumthin', 'drugpolicy', 'magnesiumglycinate', 'viral', 'blackedout', 'ecigs', 'toronto', 'medicalstudent', 'queergirl', 'lovemyself', 'substanceusedisorder', 'whiskey', 'stonersoftiktok', 'smartdrugs', 'fentfriday', 'shmoke', 'pwud', 'vitamink', 'trending', 'highasfuck', 'juul', 'philly', 'healthcareworkers', 'queertiktok', 'happytobealive', 'mentalhealth', 'cocktails', 'stonervibes', 'smartpills', 'xans', 'shmoketok', 'pwuds', 'tylenol', 'capcut', 'haf', 'cigarettes', 'midwest', 'socialworkersoftiktok', 'feralqueer', 'trauma', 'chronicpain', 'wine', 'stoner', 'cerebrolysin', 'xanny', 'keepsmokingtheherb', 'opioidawareness', 'disprin', 'duet', 'shitfaced', 'vaper', 'boston', 'socialworker', 'whitegirl', 'loveyall', 'chronicpainsufferers', 'liquor', 'laganjaestranja', 'xantok', 'drinkingtools', 'opioidcrisis', 'ayurveda', 'stitch', 'tipsy', 'cigarette', 'sanfrancisco', 'frontlineworkers', 'transtok', 'vibes', 'headache', 'tequila', '420vibes', 'xandemic', 'injection', 'overdoseawareness', 'tylenoltuesday', 'fyp„Ç∑„Çöviral', 'drunk', 'vape', 'usa', 'matnurse', 'indian', 'happy', 'migraine', 'booze', 'dabs', 'm0lly', 'injectinguser', 'overdosecrisis', 'ibuprofen', 'fyp„Ç∑', 'hammered', 'cig', 'uk', 'streetnurse', 'gaygirl', 'gratitude', 'addicted', 'chugbeer', 'wax', 'molly', 'inhaler', 'harmreduction', 'suplements', 'foryou', 'blackout', 'cigs', 'canada', 'nursetiktok', 'fear', 'bpd', 'beerfunnel', 'shmeeds', 'percs', 'inhaling', 'harmreductionworks', 'coughsyrup', 'foryoupage', 'hightimes', 'tobacco', 'therapist', 'makesomeonesmile', 'bpdtiktok', 'drinkinggames', 'percoset', 'keepsmoking', 'harmreductiontips', 'overthecounter', 'foryourpage', 'gurning', 'nicotinefein', 'addictionisreal', 'wildmandrinking', 'perc30s', 'inhalation', 'narcan', 'ibuprofeno', 'xyzbca', 'gurner', 'nicotinegum', 'ptsd', 'lean', 'narcansaveslives', 'collagen', 'trend', 'drunkaf', 'nicorettepatch', 'depression', 'leantok', 'naloxone', 'anxiety', 'acid', 'naloxonesaveslives', 'chronicpainwarrior', 'mescaline', 'safesupply', 'allergy', 'pinger', 'endoverdose', 'infection', 'pingertok', 'overdoseresponse', 'allergies', 'pingtok', 'stopsmoking', 'seizures', 'pingerzzzz', 'smokingkills', 'addictionhumor', 'drughumor', 'recoveryhumor', 'epilespy', 'emma', 'quitnicotine', 'insomnia', 'drank', 'stopvapping', 'fever', 'thatdrank', 'quitvapping', 'cough', 'bars', 'sobriety', 'adtiction', '30s', 'sober', 'asthma', '40s', 'soberliving', 'sorethroat', '512s', 'cleanandsober', 'flu', 'yellows', 'recovery', 'fluseason', 'biak', 'onedayatatime', 'bronchitis', 'epills', 'odaat', 'pain', 'psychedelictok', 'na', 'inhalantsaddiction', 'lsdart', 'detox', 'magicmushroomsadventures', 'sobrietybirthday', 'libcaps', 'soberhouse', 'libertycapss', 'wedorecover', 'thom', 'relapsehappens', 'nosebeers', 'methadoneclinic', 'c0deine', 'naltrexone', 'inhalants', 'drugrehab', 'whippets', 'dropthosedates', 'whipit', 'matsaves', 'balloons', 'stopthestigma', 'adderral', 'alcoholfreejourney', 'addy', 'alcoholic', 'alcoholism']\n",
      "\n",
      "Total number of tags: 286\n"
     ]
    }
   ],
   "source": [
    "def convert_to_tags(input_string):\n",
    "    # Split the input string into lines\n",
    "    lines = input_string.strip().split('\\n')\n",
    "    \n",
    "    # Process each line\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        # Split each line into words\n",
    "        words = line.split()\n",
    "        # Add each word as a tag\n",
    "        tags.extend(f'{word}' for word in words)\n",
    "    \n",
    "    # # Join the tags with commas\n",
    "    # result = ', '.join(tags)\n",
    "    \n",
    "    # Count the total number of tags\n",
    "    tag_count = len(tags)\n",
    "    \n",
    "    return tags, tag_count\n",
    "\n",
    "# Example input\n",
    "input_string = \"\"\"funny\tlove\tdepressionandanxiety\talcohol\tweed\tcognitiveenhancers\topioids\tsmoking\tharmreductionsaveslives\tvitamins\tfyp\tstoned\tvaping\tkensingtonphilly\tmedstudent\tlgbtqia\n",
    "addictionhumor\thope\taddiction\tbeers\tpot\tmodafinil\topiates\tdrinking\tsoberlife\tcreatine\tgreenscreen\thigh\tnicotine\tvancouverbc\tnurselife\ttransgender\n",
    "meme\tstruggle\topioidaddiction\talchohol\tcannabis\tnootropics\theroin\tdrink\tdruglaws\tmagnesium\ttiktok\twasted\tecig\tbayarea\texprostitute\tlgbtqtiktok\n",
    "\tkindness\tsubstanceabuse\tvodka\tgreens\tnootrostatic\tfent\tshmokesumthin\tdrugpolicy\tmagnesiumglycinate\tviral\tblackedout\tecigs\ttoronto\tmedicalstudent\tqueergirl\n",
    "\tlovemyself\tsubstanceusedisorder\twhiskey\tstonersoftiktok\tsmartdrugs\tfentfriday\tshmoke\tpwud\tvitamink\ttrending\thighasfuck\tjuul\tphilly\thealthcareworkers\tqueertiktok\n",
    "\thappytobealive\tmentalhealth\tcocktails\tstonervibes\tsmartpills\txans\tshmoketok\tpwuds\ttylenol\tcapcut\thaf\tcigarettes\tmidwest\tsocialworkersoftiktok\tferalqueer\n",
    "\ttrauma\tchronicpain\twine\tstoner\tcerebrolysin\txanny\tkeepsmokingtheherb\topioidawareness\tdisprin\tduet\tshitfaced\tvaper\tboston\tsocialworker\twhitegirl\n",
    "\tloveyall\tchronicpainsufferers\tliquor\tlaganjaestranja\t\txantok\tdrinkingtools\topioidcrisis\tayurveda\tstitch\ttipsy\tcigarette\tsanfrancisco\tfrontlineworkers\ttranstok\n",
    "\tvibes\theadache\ttequila\t420vibes\t\txandemic\tinjection\toverdoseawareness\ttylenoltuesday\tfyp„Ç∑„Çöviral\tdrunk\tvape\tusa\tmatnurse\tindian\n",
    "\thappy\tmigraine\tbooze\tdabs\t\tm0lly\tinjectinguser\toverdosecrisis\tibuprofen\tfyp„Ç∑\thammered\tcig\tuk\tstreetnurse\tgaygirl\n",
    "\tgratitude\taddicted\tchugbeer\twax\t\tmolly\tinhaler\tharmreduction\tsuplements\tforyou\tblackout\tcigs\tcanada\tnursetiktok\t\n",
    "\tfear\tbpd\tbeerfunnel\tshmeeds\t\tpercs\tinhaling\tharmreductionworks\tcoughsyrup\tforyoupage\thightimes\ttobacco\t\ttherapist\t\n",
    "\tmakesomeonesmile\tbpdtiktok\tdrinkinggames\t\t\tpercoset\tkeepsmoking\tharmreductiontips\toverthecounter\tforyourpage\tgurning\tnicotinefein\t\t\t\n",
    "\t\taddictionisreal\twildmandrinking\t\t\tperc30s\tinhalation\tnarcan\tibuprofeno\txyzbca\tgurner\tnicotinegum\t\t\t\n",
    "\t\tptsd\t\t\t\tlean\t\tnarcansaveslives\tcollagen\ttrend\tdrunkaf\tnicorettepatch\t\t\t\n",
    "\t\tdepression\t\t\t\tleantok\t\tnaloxone\t\t\t\t\t\t\t\n",
    "\t\tanxiety\t\t\t\tacid\t\tnaloxonesaveslives\t\t\t\t\t\t\t\n",
    "\t\tchronicpainwarrior\t\t\t\tmescaline\t\tsafesupply\t\t\t\t\t\t\t\n",
    "\t\tallergy\t\t\t\tpinger\t\tendoverdose\t\t\t\t\t\t\t\n",
    "\t\tinfection\t\t\t\tpingertok\t\toverdoseresponse\t\t\t\t\t\t\t\n",
    "\t\tallergies\t\t\t\tpingtok\t\tstopsmoking\t\t\t\t\t\t\t\n",
    "\t\tseizures\t\t\t\tpingerzzzz\t\tsmokingkills\taddictionhumor\n",
    "drughumor\n",
    "recoveryhumor\t\t\t\t\t\t\n",
    "\t\tepilespy\t\t\t\temma\t\tquitnicotine\t\t\t\t\t\t\t\n",
    "\t\tinsomnia\t\t\t\tdrank\t\tstopvapping\t\t\t\t\t\t\t\n",
    "\t\tfever\t\t\t\tthatdrank\t\tquitvapping\t\t\t\t\t\t\t\n",
    "\t\tcough\t\t\t\tbars\t\tsobriety\t\t\t\t\t\t\t\n",
    "\t\tadtiction\t\t\t\t30s\t\tsober\t\t\t\t\t\t\t\n",
    "\t\tasthma\t\t\t\t40s\t\tsoberliving\t\t\t\t\t\t\t\n",
    "\t\tsorethroat\t\t\t\t512s\t\tcleanandsober\t\t\t\t\t\t\t\n",
    "\t\tflu\t\t\t\tyellows\t\trecovery\t\t\t\t\t\t\t\n",
    "\t\tfluseason\t\t\t\tbiak\t\tonedayatatime\t\t\t\t\t\t\t\n",
    "\t\tbronchitis\t\t\t\tepills\t\todaat\t\t\t\t\t\t\t\n",
    "\t\tpain\t\t\t\tpsychedelictok\t\tna\t\t\t\t\t\t\t\n",
    "\t\tinhalantsaddiction\t\t\t\tlsdart\t\tdetox\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tmagicmushroomsadventures\t\tsobrietybirthday\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tlibcaps\t\tsoberhouse\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tlibertycapss\t\twedorecover\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tthom\t\trelapsehappens\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tnosebeers\t\tmethadoneclinic\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tc0deine\t\tnaltrexone\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tinhalants\t\tdrugrehab\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twhippets\t\tdropthosedates\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twhipit\t\tmatsaves\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tballoons\t\tstopthestigma\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tadderral\t\talcoholfreejourney\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\taddy\t\talcoholic\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\talcoholism\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"\n",
    "\n",
    "# Convert and print the result\n",
    "all_human_tags, count = convert_to_tags(input_string)\n",
    "print(\"Converted tags:\")\n",
    "print(all_human_tags)\n",
    "print(f\"\\nTotal number of tags: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of nodes: 2333\n",
      "Total count of nodes after removal: 2150\n",
      "Remaining nodes: ['peopleoverprofit', 'harmreductiontiktok', 'b', '4u', 'greenscreenvideo', 'signs', 'medications', 'us', 'virall', 'fentanyleducation', 'mentallyill', 'clips', 'nurse', 'alcoholaddiction', 'education', 'paradise', 'addicthumor', 'schizoaffective', 'amazon', 'bestie', 'addictedseries', 'motivational', 'crank', 'indigenoustiktok', 'pretty', 'adhd', 'البحرين', 'nonbinary', 'smallbusiness', 'hcvtesting', 'child', 'save', 'footballs', 'recoverycomedy', 'beach', 'juice', 'wellbriety', 'comedian', 'bipolarawareness', 'daily', 'louisiana', 'painadvocate', 'teens', 'vitaminr', 'addictionmedicine', 'losangeles', 'qatar', 'autism', 'retokfornature', 'endprohibition', 'chevy', 'trump', 'growthmindset', 'mexico', '❤️', 'cbd', 'five', 'invisibleillness', 'prohibitionispoison', 'strength', 'hazbinhotel', 'fitness', 'ventana', 'hard', 'painpatient', 'genshinimpact', 'fearmongering', 'firefighter', 'bowls', 'drugpolicyreform', 'bikelife', 'disability', 'friday', 'quitsmoking', 'spun', 'rehabcenter', 'recoveryunplugged', 'domesticviolencesurvivor', 'opiateawareness', 'blanket', 'legalize', 'fupシ', 'parati', 'rock', 'custom', 'opioidpainmedicine', 'fypdong', 'fypsoberlife', 'bamf', 'traumahealing', 'counseling', 'schizophrenic', 'flowers', 'mushrooms', 'grass', 'atrain', 'suboxon', 'depressed', 'herb', 'addictionisntfun', 'substanceabuseproblem', 'noway', 'opiatesurvivor', 'meow', 'thc', 'paindoctor', 'metal', 'aviation', 'syringeexchange', 'humanrights', 'accountabro', 'methadoneawareness', 'comedia', 'wolfies', 'narcansavedmylife', 'positivevibes', 'insta', 'coffee', 'poison', 'cooking', 'vivitrol', 'edc', 'nosepowder', 'italia', 'soberafvibes', 'fypdongggggggg', 'emotionalsobriety', 'bpdawareness', 'matrix', 'pnp', 'clouds', 'kid', 'brother', 'multiplepathwaysofrecovery', 'pets', 'fyy', '🍻', 'carrynaloxone', '5', 'veteransaffairs', 'overdose', 'boozefree', 'sobermomsoftiktok', 'bigh', 'sexton', 'methadon', 'overdos', 'nursesoftiktok', 'sobrietyrockstars', 'cptsdawareness', 'inrecovery', 'work', 'thankful', 'wood', '12stepsinrecovery', 'and', 'educational', 'rollin', 'dr00gtiktok', 'sunday', 'wedorevover', 'letsgo', 'manga', 'dc', 'effects', 'artwork', 'werecoverstronger', 'japan', 'dúo', 'nursing', 'cutecat', 'skittles', 'ae', 'yoga', 'skit', 'overdoseawarenessday', 'saynotodrugs', 'disabledtiktok', 'health', 'dvsurvivor', 'ouidtiktok', 'memesdaily', 'readysetlift', 'theafterparty', 'batu', 'windowpane', 'bipolar1', 'ecigarettes', 'festivalseason', 'eliquid', 'ravetok', 'benzogang', 'the', 'dontdodrugs', 'drvgaddiction', 'powder', 'quitvaping', 'warondrugs', 'mrnice', 'drugrecovery', 'funnyy', 'crps', 'ninja', 'sobersquad', 'reds', 'fuckdrugs', 'goals', 'matsavedme', 'soberdogs', 'maryjane', 'florida', 'needle', 'happybirthday', 'painkillers', 'fürdich', 'drugs', 'fall', 'crime', 'stonertokfyp', '100k', 'motorcycle', 'addictionmindset', 'heaven', 'sadtok', 'lilpeep', 'podcast', 'savelives', 'dogs', 'asmr', 'trippycontent', 'fentfold', 'syrup', 'nativetiktok', 'narcanawareness', 'club', 'sports', 'cleantok', 'pharmacy', 'tw', 'supportdontshame', 'addictionisrealthereishope', 'andgo', 'sundayfunday', 'bunkpolice', 'yucatan', 'artist', 'benzowithdrawal', 'chinagirl', 'filmteyvatislands', 'ndukulu', 'stopdrinking', 'disease', 'truck', 'misinformation', 'vela', 'foryourpages', 'laughing', 'compassion', 'patientadvocate', 'animalsoftiktok', 'couple', 'drugpolicies', 'عمان', 'prescription', 'relapse', 'follow', 'ireland', 'cannafamily🌿', 'harmreductionmythbuster', 'womenover40', 'research', 'r2', 'tranq', 'addictionsurvivor', 'sobrietyisbeautiful', 'gaming', 'opioiddetox', 'keşfet', 'vaped', 'inspirational', 'addictsoftiktok', 'childhoodtrauma', 'werecover', 'matrecovery', 'sobergirl', 'sublocadeshot', 'gas', 'mama', 'fyprecovery', 'mindset', 'natural', '4you', 'vapelife', 'arachnoiditis', 'addictionskit', 'youcandoit', 'publichealth', 'ping', 'basketball', 'guma', 'bcbud', 'funnyvideos', 'autotweet', 'legal', 'cannafam', 'midnightoil', 'ineedhelp', 'recoverycommunity', 'endstigma', 'aesthetic', 'arte', 'addictionfree', 'medicineexplained', 'justfortoday', 'guardians', 'hair', 'sisters', 'tattoo', 'seshtok', 'rush', 'hows', 'cinema', 'xuhuong', 'dj', 'illicitfentanylkills', 'harmreductionbaddietok', 'matrecoveryadvocate', 'tampabay', 'anime', 'mefftok', 'safesupplysaveslives', 'winter', 'soccer', 'goodmorning', 'breakthestigma', 'yummy', 'outreachworker', 'chill', 'pride', 'stigmakills', 'dank', 'herbalmedicine', '4youpage', 'spinalcordinjury', 'dead', 'poetry', 'woundcare', 'crisis', 'skee', 'depresion', 'motherofanaddict', 'myjourney', 'test', 'gacha', 'ssp', 'dopesick', 'toxic', 'fittok', 'skippy', 'methtok', 'ontario', 'deadpeoplecantrecover', 'dryjanuary', 'decriminalization', 'psychosis', 'fast', 'vapetricks', 'kpop', 'acutepain', 'tiktokviral', 'knowyourstatus', 'china', 'mindfulness', 'friend', 'рекомендации', '❄️tok', 'doctor', 'peoriaaz', 'god', 'lgbtq', 'negra', 'zaza', 'xylazine', 'fy', 'xan', 'illustration', 'dontletthisflop', 'm30', 'newyear', 'volunteer', 'whiteknight', 'hcv', 'depressionanxiety', 'blacktar', 'garden', 'unitedstates', 'za', 'withdrawals', 'turbo', 'confidence', 'endthewarondrugs', 'footballtiktok', 'friends', 'tv', 'buffalo', 'movie', 'planks', '🧊', 'sleepy', 'dogsoftiktok', 'gang', 'pingin', 'freedom', 'cotton', 'foryoupageofficiall', 'onthisday', 'romance', 'england', 'beat', 'sign', 'cptsd', 'disney', 'opioidpainmedicinesaveslives', 'notalone', 'mentalillness', 'run', 'for', 'australia', 'michigan', 'ny', 'abby', 'nursetok', 'tweaker', 'training', 'onepillcankill', 'islam', 'drone', 'sublocadejourney', 'uae', 'crazyclown', 'breakup', 'likes', 'we_do_recover', 'fentanyawarness', 'addicttok', 'travel', 'infectiousdisease', 'war', '🐴', 'in', 'brasil', 'dayonedetox', 'healthy', 'support', 'j', 'unleashyourgods', 'games', 'caffeine', 'methadonemaintenance', 'narcoticsanonynous', 'dlaciebie', 'chile', 'psychedelic', 'storytime', 'drugusersdeserveempathy', 'recoveryjourney', 'ai', 'bipoc', 'newzealand', 'prohibitionkills', 'proud', '12step', 'buprenophrhine', 'laugh', 'medicaldetox', 'creator', 'naloxonesavedmylife', 'study', 'soberflex', 'single', 'catlover', 'quitting', 'grunge', 'neurofibromatosis', 'humor', 'hellokitty', 'gee', 'endthestigma', 'fbi', 'ukraine', 'british', 'spreadawareness', 'addictiontiktok', 'beauty', 'xandemicsurvivor', 'trippingballs', 'minnesota', 'funnymoments', 'recoverylife', 'goodfellas', 'activeuse', 'cancer', 'dawn', 'ootd', 'motivation', 'swag', 'gato', 'peoplewhousedrugs', 'paratii', 'american', 'edutok', 'yellow', 'queen', 'aa', 'money', 'jokes', 'straightedge', 'soberity', 'alcoholfree', 'gettingclean', 'weekend', 'spike', 'memepage', 'prayers', 'pill', 'borderlinepersonalitydisorder', 'bronx', 'rip', 'md', 'snow', 'a', 'steps', 'makethisviral', 'discover', 'skoobysnax', 'herniateddisc', 'psychedelicresearch', 'heman', 'movewithtommy', 'plug', 'awareness', 'fire', 'mentalwellness', 'veteran', 'addictionisnthowyourstoryhastoend', 'fypmentalhealth', 'mentalhealthadvocate', 'beyourself', 'reefer', 'dancefever', 'imissyou', 'specialk', 'tired', 'thankyou', 'wafer', 'vapeon', 'amor', 'opioidhysteria', 'summer', 'mentalhealthrecovery', 'cover', 'opiateskill', 'clarity', 'depressionisreal', 'selfimprovement', 'positivity', 'dreamer', 'explorepage', 'p1ng', 'withdrawls', 'islamic', 'influencer', 'job', '420', 'asmrtiktoks', 'stress', 'hippie', '710', 'sobermovement', 'mentalhealthmatters', 'gravel', 'cloudnine', 'lifehacks', 'bipolar1disorder', 'pan', 'triplec', 'overdoseawarenessmonth', 'movies', 'mentalillnessinnit', 'food', 'concert', 'spirituality', 'u4', 'carsoftiktok', 'emergency', 'advocacymatters', 'humour', 'genx', 'soberaf', 'chat', 'grwm', 'stimulants', 'theplug', 'diy', 'sigma', 'realliferecovery', 'genie', 'ocean', 'nursingstudent', 'rescuebreathing', 'recoveringaddictsoftiktok', 'riseupglendale', 'therealpussinboots', 'free', 'supportgroup', 'wildlife', 'mutualaid', 'truecrime', 'petsoftiktok', '❄️', 'xtc', 'spiritualtiktok', 'pov', 'riseupaz', 'addictionjourney', 'learnontiktok', 'indo', 'vapingisbad', 'addictionsupport', 'whitelightning', 'america', 'salud', 'bluecollar', 'fentanyloverdose', 'e', 'ford', 'neverusealone', 'hydro', 'hash', 'creepy', 'nightout', 'fypp', 'droog', 'mtv', 'genshinteleport', 'challenge', 'rapper', 'getcrackin', 'oxy', 'narcansaves', 'moda', 'oregon', 'editor', 'christiantiktok', 'gethelp', 'doc', 'reducingharm', 'addictshelpingaddicts', 'cia', 'kitty', '70s', 'edrecocery', 'sky', 'ems', 'backpainexercises', 'birthday', 'alttiktok', 'boy', 'm', 'r', 'roblox', 'street', 'sask', 'hungover', 'lowbrowcomedy', '2', 'car', 'awarenessvideo', 'fentynalawareness', 'familytime', 'green', 'spain', 'oklahoma', 'uni', 'bennies', 'handmade', 'dots', 'cdc', 'pills', 'showofflandofrost', 'jointpain', 'motivationalquotes', 'newmusic', 'genshinimpact32', 'lmao', 'hippietok', '2000s', 'carrynarcan', 'zannies', 'family', 'thisisquitting', 'knowledge', 'share', 'cripple', 'life', 'drugdetox', 'withdrawal', 'dmt', 'foryouu', 'manchester', 'tree', 'zyxcba', 'sad', 'deep', 'sicklecell', 'supportdontpunish', 'seehergreatness', 'beautiful', 'led', 'twitch', 'v', 'halloween', 'air', 'success', 'vs', 'cake', 'fentanyldangersarereal', 'fashiontiktok', 'substances', 'matsavedmylife', 'joint', 'haddict', 'ghb', 'gains', 'dog', 'photo', 'people', 'keepgoing', 'accountability', 'christianity', 'growth', 'recovering', 'comment', 'matsaveslives', 'odawareness', 'c', 'africa', 'sobrietytiktok', 'illicitfentanyl', 'plant', '90s', 'tok', 'unhoused', 'sobercoach', 'rehabilitation', 'cigar', 'xzybca', 'alcoholrehab', 'painful', 'pharmacology', 'thursday', 'lenovojustbeyou', 'skunk', 'chs', 'fact', 'blow', 'justice', 'traphouse', 'alternative', 'bestfriends', 'psa', 'art', 'addictionhelp', 'edit', 'sinsemilla', 'drugabuse', 'courage', 'stardust', 'alcoholicsanonymous', 'trippyvideos', 'hangover', 'howtogetsober', 'supplements', 'artistsoftiktok', 'cdcguidelines', 'рек', 'france', 'pregnancy', 'shorts', 'methadonemaintenace', 'kawaii', 'alt', 'mentalhealthawarness', 'learning', 'f', 'heal', 'jesus', 'turkey', 'drugsafety', 'wife', 'safespace', 'new', 'smokers', 'meditation', '🍄', '4k', 'mixing', 'messi', 'harmreductionist', 'drugoverdose', 'er', 'gofast', 'empathy', 'trigeminalneuralgia', 'hell', 'painrelief', 'bootsontheground', 'bangi', 'pinterest', 'kensingtonbeach', 'sweet', 'relateable', 'methadoneisrecovery', 'gettested', 'endhivstigma', 'newyork', 'rave', 'headaches', 'standup', 'methadonesavedmylife', 'themoreyouknow', 'druginducedhomicide', 'wellness', 'selflove', 'recipe', 'outdoors', 'marlboro', 'activeaddictionvssobriety', 'iloveyou', 'holiday', 'ohio', 'opioidaddictionrecovery', 'werecovermfr', 'makethisblowup', 'gratefuladdictinrecovery', 'story', 'film', 'eccies', 'laughter', 'sobertok', 'psychology', 'suboxoneisrecovery', '3', 'downers', 'techno', 'rec', 'newyorkcity', 'shoulderpain', 'king', 'entrepreneur', 'likeabombshell', 'holidayoreoke', 'addictionawareness', 'joke', 'lesbian', 'thecolemaninstitute', 'reels', 'recover', 'iphone', 'arnolds', 'nice', 'outpatient', 'medicationassistedtreatment', 'followme', 'recoveryfam', 'friendship', 'women', 'mentalhealthtok', 'thenvsnow', 'vlog', 'dumbjokes', 'cravings', 'viraltiktok', 'activeaddictiontorecovery', 'up', 'tuesday', 'pellets', 'postitaffirmations', 'water', 'drvgzx', 'activeaddiction', 'grief', 'drvgadicction', 'stayalert', 'addicts', 'sh', 'chronicillnesswarrior', 'staysober', 'singer', 'druguse', 'bipolardisorder', 'strong', 'zero', 'aesthetics', 'higherpower', 'lifelessons', 'safeuse', 'sobernation', 'makethisgoviral', 'downtowntampa', 'sa', 'vegan', 'karen', 'lucy', 'reddove', 'addict', 'dream', 'purplewave', 'endmassincarceration', 'fypviralシ', 'day', 'manifest', 'makemefamous', 'cats', 'jump', 'addictionstory', 'producer', 'mandy', 'auntmary', 'bobo', 'please', 'arizona', 'sublocade', 'loss', 'recoverycheck', 'opiate', 'painhub', 'dreamgun', 'newtrend', 'instagram', 'sobercurious', 'advice', 'drugusersarepeopletoo', '1', 'fentnyltest', 'jackpot', 'prison', 'dobetter', 'blessed', 'glendaleaz', 'tik_tok', 'safety', 'helpthisgetonfyp', 'capcutvelocity', 'content', 'migrainerelief', 'detroit', 'painting', 'texas', 'paintiktok', 'sketch', 'vidrio', 'propaganda', 'voorjou', 'satisfying', 'safetyfirst', 'night', 'supportnotstigma', 'roach', 'joystick', 'parents', 'tutorial', 'ocd', 'dontdodrxgs', 'stims', 'wednesday', 'harmreductionspecialist', 'getintheway', 'addictionisnotachoice', 'gift', 'genxcrew', 'broken', 'fup', 'pingtok⚫️👄⚫️', 'pop', 'indigenous', 'pingtok🤭💊', 'followforfollowback', 'truckdrivers', 'photography', 'kr8tom', 'chadsabora', 'animals', 'sadstory', '3days', 'ehlersdanlossyndrome', 'jollygreen', 'abcxyz', 'catchchobanioatmilk', 'bhang', 'morningroutine', 'meffaddiction', 'neckpain', 'recoverytok💜', 'hilarious', 'mushroom', 'stackers', 'methadonetaper', 'fenty', 'lapse', 'lungs', 'fail', 'queer', 'momsinrecovery', 'sublocadeinjection', 'benzo', 'inspire', 'toyota', 'migraines', 'haha', 'military', 'no', 'vintage', 'perc30', 'makenightsepic', 'puffpuffpass', 'oddlysatisfying', 'fentanylkills', 'magic', 'raver', 'bff', 'sud', 'socialwork', 'perte', 'hiphop', 'helpthisgoviraltiktok', 'fypviral', 'tik', 'explore', 'lives', 'actor', 'reddit', 'prayer', 'pink', 'matproud', 'ehlersdanlos', 'boss', 'helpful', 'awakening', 'fibromyalgia', 'sport', 'anxietyrelief', 'substanceuseawareness', 'healthadepopit', 'tarot', 'charlie', 'pumpers', 'drunktok', 'جده', 'seshlife', 'allah', 'dnb', 'pingertok💊', 'binge', 'vapoter', 'syringeservicesprogram', 'endthedrugwar', 'youtube', 'oil', 'darkhumor', 'pharmacist', 'you', 'coach', 'sobertiktok', 'bike', 'soberhumor', 'spinalcordstimulator', 'cpr', 'nightlife', 'mrniceguy', 'genshinimpact33', 'nike', 'opioid', 'harmreductionworker', 'holidays', 'harmreductionislove', 'snowleopard', 'healthcare', 'instagood', 'tina', 'opioidendemic', 'realtalk', 'shadowpeople', 'today', 'paintok', 'mira', 'vapor', 'makethisvideogoviral', '💊', 'drawing', 'marvel', 'standupcomedy', 'recoveryfamily', 'junkies', 'nod', 'recoveryqueen', 'loud', 'momtok', 'theafters', 'sticks', 'enabling', 'cleantime', 'college', 'detoxing', 'soberlution', 'build', 'mat', 'ed', 'disabledcommunity', 'raves', 'gear', 'starwars', 'amps', 'faith', 'baby', 'relapseprevention', 'barbs', 'alcoholicsaononymous', 'alcoholicsnotanonymous', 'werecoverwell', 'on', 'sobrietyrocks', 'spoonie', 'like', 'lyrics', 'realtalkrecovery', 'blowitup', 'foryoupage❤️❤️', 'munchies', 'narcotics', 'lovely', 'recoverytok', 'addictsarepeopletoo', 'spooniesoftiktok', 'construction', 'page', 'xybca', 'policyreform', 'trash', 'broccoli', 'kif', 'dance', 'wedorecovery', 'learn', 'atx', 'dr✨️gs', 'saferuse', 'addictioncrew', 'bipolar', 'helpthisgoviral', '18', 'mountains', 'overdosed', 'cold', 'staysafe', 'chicago', 'easy', 'mota', 'الكويت', 'sucht', 'missyou', 'drugfacts', 'football', 'norway', 'medicatedassistedtreatment', 'focus', 'السعودية', 'mellowyellow', 'trippyart', 'mollytokk', 'california', 'girl', 'haddiction', 'bar', 'hops', 'getsober', 'christmas', 'naloxonetraining', 'hospital', 'anesthesiologist', 'style', 'chalk', 'recoveryjournal', 'bullying', 'law', 'addictionisamentalhealthproblem', 'fentanylcrisis', 'sublocadequestions', 'monster', 'earth', 'facebook', 'comedancewithme', 'icetok', 'athlete', 'recoveryispossible', 'y', 'bible', 'opiod', 'policy', 'depressedtiktok', 'futbol', 'ccc', 'borderline', 'tabac', 'loveislove', 'cut', 'ice', 'rich', 'weightloss', 'disabilitytiktok', 'apache', 'drugcrisis', 'lgbt', 'ms', 'memes', 'covid', 'over40', 'spiritualawakening', 'wedorecoverfam', 'model', 'tumors', 'halfwayhouselife', 'nomoredrugwar', 'recoveryreform', 'cpp', 'blockbusters', 'backpain', 'musician', 'relax', 'death', 'chronicpainawareness', 'colemanmethod', 'junkie', 'inspiration', 'opiods', 'photoshoot', 'womenownedbusiness', 'ddd', 'sleep', 'serenity', 'edmtiktok', 'circles', 'naruto', 's', 'india', 'voiceeffects', 'neuroscience', 'parentsoftiktok', 'redx', 'venting', 'coca', 'woman', 'wales', 'savage', 'surgery', 'fighter', 'sence', 'fda', 'stovetop', 'nic', 'famous', 'comedу', 'killers', '🔌', 'london', 'ashes', 'trap', 'drugeducation', 'safeconsumptionsites', 'purple', '3pcoalition', 'legend', '2022', 'home', 'wdr', 'jail', 'cars', 'alien', 'joysmoke', 'dayinmylife', 'top5', 'argentina', 'opioidsaresafe', 'drugtreatment', 'felon', 'sobrietyispossible', 'kindukulu', 'brain', 'overdoseprevention', 'eve', 'kitten', 'theboys', 'chronicallyill', 'nfl', 'testyoursubstances', 'xyzabc', 'hivprevention', 'drugfree', 'salvia', 'vaporizer', 'politics', 'breakfast', 'antidepressants', 'addictionrecovery', 'xylazinekills', 'emotional', 'vancouver', 'goodvibes', 'vapejuice', 'autistic', 'backsurgery', 'crazy', 'quitdrinking', 'look', 'pinky', 'fake', 'music', 'recoveryisworthit', 'drugwar', 'decriminalize', 'fyppppppppppppppppppppppp', 'dankmemes', 'red', 'oldschool', 'nursepractitioner', 'yum', 'fanart', 'toxicology', 'lovequotes', 'cvspaperlesschallenge', 'neurodivergent', 'scary', 'fypgakni', 'n', 'series', 'core', 'eminem', 'needleexchange', 'inverted', 'crystals', 'subxone', 'dontpunishpain', 'fr', 'quote', 'bud', 'canna', 'addictiontok', 'relationship', 'crash', 'sniff', 'dejardefumar', 'euphoria', 'inpatient', 'tiktokaddict', 'treatmentcenter', 'medicine', 'yellowstonetv', 'barbie', 'rollbowl', 'october', 'wtf', 'time', 'featureme', 'viral_video', 'coke', 'wow', 'thesesh', 'singing', 'candycrush10', 'chronicpainlife', 'colombia', 'couplegoals', 'pipe', 'news', 'picoftheday', 'momsoftiktok', 'biscuit', 'sobercouple', 'mcyt', 'mentalhealthawareness', 'd', 'chevyevsongcontest', 'h', 'indonesia', 'sweden', 'blue', 'sobrietyisworthit', 'vapedaily', 'believe', 'yaba', 'likeforlikes', 'makeup', 'festival', 'chronic', 'addictinrecovery', 'staysavage', 'gamer', 'manners', 'elfbar', 'bag', 'boyfriend', 'spice', 'endocarditis', 'xyz', 'chandu', 'narcoticsanonymous', 'm0llytikt0k', 'marriage', 'meowmeow', 'dubstep', 'fypsounds', 'drake', 'effect', 'video', 'snap', 'smarties', 'bekind', 'crazystory', 'oc', 'emo', 'drugpolicyalliance', 'fnaf', 'zohai', 'teststrips', 'witchcraft', 'adhdtiktok', 'bliss', 'fender', 'taylorswift', 'depressionawareness', 'recoveryquotes', 'goop', 'one', 'bodybuilding', 'passion', 'addictiontreatment', 'healthcareworker', 'fry', 'painmanagement', 'tampaflorida', 'intractablepain', '8ball', 'bricks', 'black', 'educationalpurposes', 'cactus', 'forupage', 'lupus', 'rapiddetox', 'worthsaving', 'reality', 'viralvideo', 'relatable', 'overamping', 'fakesituation', 'hiv', 'country', 'blackownedbusiness', 'fifa', 'hashtag', 'actress', 'church', 'add', 'stp', 'howtogetclean', 'blowup', 'blackmagic', 'fypage', 'good', 'catha', 'system', 'power', 'methadonemile', 'u', 'mylife', 'gabapentin', 'grievingmom', 'indie', 'smoke', 'musically', 'guitar', 'safesupplyorwedie', 'addictsinrecovery', 'nightmare', 'jesussaves', 'unhousedcommunity', 'morning', 'dancewithturbotax', 'nourisheveryyou', 'uppers', 'heartbroken', 'nonprofit', 'plants', 'cancerpain', 'punk', 'relief', 'intervention', 'demon', 'cool', 'children', 'netflix', 'sobrietycheck', 'mood', 'speakers', 'miraa', 'eat', 'muchachas', 'help', 'drama', 'gondola', 'bpdtok', 'photooftheday', 'handlebars', 'midnight', 'rehab', 'la', 'disabled', 'prohibition', 'recoveroutloud', 'benzodiazepinas', 'العراق', 'soberjourney', 'booktok', 'recoveroutloud💜', 'iykyk', 'alcoholfreelifestyle', 'dr00ghumour', 'savealife', 'alcoholtreatment', 'truestory', 'wwe', 'bmw', 'desert', 'opiatewithdrawal', 'opioidsareneccessary', 'drugawareness', 'raisingawareness', 'reachout', 'top', '💉', 'colemaninstitute', 'trendy', 'quoteoftheday', 'activeaddictionstories', 'irish', 'godsmedicine', 'ideaexchange', 'vapepen', '2023', 'vent', 'perc10challenge', 'draft', 'easywithadobeexpress', 'k', 'spinabifida', 'copsoftiktok', 'glass', 'blackbeauties', 'community', 'prisontok', 'werecovertogether', 'kicker', 'foryour', 'methamphytamine', 'silly', 'advocacy', 'truth', 'angel', 'lmfao', 'catsoftiktok', '12steps', 'degenerativediscdisease', 'monday', 'real', 'medicaltiktok', 'actuallyautistic', 'smack', 'medication', 'fashion', 'takeanairbreak', 'neiperte', 'workout', 'kat', 'depressionhelp', 'apple', 'spiritual', 'dr00g', 'opiatedetox', 'answer', 'bhfyp', 'buprenorphinesaveslives', 'thesinclairmethod', 'flake', 'minecraft', 'robo', 'genshinimpact34', 'substanceabuser', 'wlw', 'idea', 'darkhumour', 'aussie', 'watch', 'world', 'matisrecovery', 'addictionisadisease', 'soberbaddie', 'veterans', 'x', 'perc', 'kitkat', 'anesthesia', 'fentanylfacts', 'crystal', 'harmredux', 'teenager', 'momlife', 'toys', 'retro', 'mc', 'drvg', 'breakthecycle', 'housingfirst', 'oud', 'mujeres', 'yougotthis', 'functioningaddict', 'apace', 'history', 'pourtoi', 'lovinglife', 'positive', 'cat', 'doctorsoftiktok', 'opioidepidemic', 'shards', 'thebag', 'mystory', 'alone', 'blues', 'smile', 'fortnite', 'juicewrld', 'satire', '716buffalo', 'repost', 'sobrietytok', 'trip', 'k2', 'light', 'fendt', 'videoviral', 'omg', 'spreadingawareness', 'painkiller', 'dark', 'trippin', 'memories', 'roids', 'griefjourney', 'chronicillness', 'fentanylpoisioning', 'afters', 'psy', 'happiness', 'endlessjourney', 'spinesurgery', 'sublocadeexperience', 'tripping', 'doctors', 'jailtok', 'recoveringoutloud', 'cheeba', 'digitalart', 'ladders', 'goviral', 'breakingnews', 'reddevils', '🍃tiktok', 'science', 'joerogan', 'house', 'me', 'gasper', 'nostalgia', 'lgbt🌈', 'xylazinetesting', 'fit', 'outreach', 'sesh', 'amped', 'puppy', 'w', 'loveyou', 'substanceuse', 'asmrvideo', 'gentleman', '🍃', 'grateful', 'earlysobriety', 'best', 'medtok', 'partytime', '4upage', 'copingmechanism', 'gohelpsomeone', 'ghost', 'another24', 'justintrudeau', 'l', 'cartoon', 'xylazineawareness', 'of', 'meds', 'color', 'fasttwitchcontest', 'hot', 'relationships', 'opiateepidemic', 'godisgood', 'od', 'game', 'bass', 'bestfriend', 'energy', 'gay', 'dadsoftiktok', 'harm', 'beans', 'mom', 'registerednurse', 'tiktokindia', 'autisticadult', 'classic', 'foru', 'her0in', 'guy', 'prank', 'gratefuladdictsinrecovery', 'spinehealth', '2021', 'soberflexfam', 'educationalcontent', 'lol', 'soberissexy', 'crystaltok', 'spotify', 'getthebag', 'addictionisrealrecoveryisbeautiful', 'hulk', 'traumatok', 'throwback', 'christian', 'english', 'vanillasky', 'live', 'bomber', 'fetty', 'naloxonesaves', 'experience', 'tattoos', 'velvet', 'sobercomedy', 'syringeaccess', 'homeless', 'gymtok', 'lohmamaof4', 'amazonvirtualtryon', 'healingjourney', 'hivawareness', 'selfie', 'newlife', 'activeaddictionthings', 'sublocadecommunity', 'fyyyyyyyyyyyyyyyy', 'loved', 'alcoholfreelife', 'opiateaddictionawareness', 'wellbeing', 'recoveryhouse', 'bicycleparts', 'recoveryarmy', 'رمضان_كريم', '12stepprogram', 'gym', 'selfcare', 'paint', 'harmreductiongang', 'overdosawarenes', 'audio', 'lanadelrey', 'fly', 'goofballs', 'comediahumor', 'opiodepidemic', 'soberlifestyle', 'ope', 'addictionrecoverycoach', 'wedorecoverwecanrecover', 'stayalive', 'luxury', 'muslim', 'psychosisawareness', 'highschool', 'fyppp', 'songforcharlie', 'therapy', 'entertainment', 'fentanylawareness', 'schoolbus', 'dope', 'helping', 'musicfestival', 'kentucky', 'cbs', 'blackwomenowned', 'chronsdisease', 'sciatica', 'adam', 'saturday', 'lifestyle', 'dust', 'mattok', 'ketaminetiktok', 'europe', 'drugabuseawareness', 'legalizeit', 'dayinthelife', 'man', 'horror', 'animal', 'goat', 'snapchat', 'sobrietyjourney', 'clubbing', 'kinen', 'peace', 'naloxonekit', 'sick', 'addictionisrealyall', 'dancesafe', 'dothework', 'joy', 'kids', 'needles', 'helpme', 'vacation', 'bro', 'transformation', 'beer', 'prevention', 'multiplesclerosis', 'recoverycoach', 'opioidoverdoseprevention', 'army', 'edrecovery', 'shots', 'الامارات', 'op', 'overdosawareness', 'canadian', 'funnymemes', 'downer', 'chinatown', 'epidemic', 'hi', 'addictionstories', 'freefromaddiction', 'opiaterecovery', 'boundaries', 'substanceabuserecovery', 'usatiktok', 'party', 'information', 'harmreductionsavealives', 'workthesteps', 'amazing', 'fypシ゚viral', 'psychedelicart', 'cute', 'egypt', 'police', 'tangoandcash', 'tampa', 'afterparty', 'stigma', 'i', 'addictcomedy', 'overwatchme', 'rap', 'humanity', 'dangerous', 'matissober', 'ze', 'camping', 'dea', 'g', 'drinks', 'germany', 'hillsborough', 'dothesmartthings', 'matawareness', 'physicianassistant', 'lifesaving', 'celebrity', 'suboxonedetox', 'postopsurgery', 'alcoholabuse', 'ventaccount', 'heart', 'interview', 'grasstok', 'homelessness', 'dosewithme', 'naturesmedicine', 'fypageシ', 'opiatecrisis', 'ravers', 'arab', 'healingtiktok', 'bts', 'business', 'whitedove', 'buttons', 'memecut', 'z', 'plur', 'healthyliving', 'recoveroutloudforthosewhosufferinsilence', 'drumandbass', 'prisontiktok', 'detoxification', 'ejuice', 'topgunmode', 'xyzcba', 'edits', 'opioidrecovery', 'charlidamelio', 'highlife', 'suboxonecheck', 'recovered', 'sobrietymemes', 'foodie', 'speed', 'crack', 'opiateusedisorder', 'chicken', 'yxe', 'comedyvideo', 'change', 'rescue', 'struggles', 'philadelphia', 'werecoverasone', 'fypシ', 'dubai', 'plantmedicine', 'future', 'wisdom', 'sobermom', 'girls', 'witchtok', 'shroooms', 'cleaning', 'brand', 'white', 'narcantraining', 'copilots', 'popular', 'soberfamily', 'opiateaddictionrecovey', 'usa_tiktok', 'postoppain', 'oat', 'suboxonesaveslives', 'question', 'horse', 'rolling', 'lines', 'lpn', 'healing', 'dontdodrugs❌', 'fight', 'meff', 'hop', 'qat', 'peoplewhouse', 'chronicillnessawareness', 'kneepain', 'smoker', 'trans', 'pet', 'sunset', 'angry', 'roxy', 'wecanrecover', 'design', 'hollywood', 'withdrawalsymptoms', 'wearetgh', 'tghcares', 'lies', 'jesuschrist', 'icecream', 'tiktokers', 'primedaydreamdeals', 'loveyourself', 'healthylifestyle', 'asmrsounds', 'pingtok💊', 'pa', 'corinnadoll', 'nevergiveup', 'old', 'blowthisup', 'flexeveryangle', 'gangster', 'every11minutes', 'global', 'vapefam', 'comedy', 'bigpharma', 'scotland', 'opiatesarethedevil', 'exercise', 'quotes', 'underground', 'edm', 'student', 'theplate', 'animation', 'vibe', 'overcomingaddiction', 'trippy', 'mensmentalhealth', 'calisober', 'slideshow', 'nurses', 'bikerscoffee', 'addictedtotiktok', 'howtoquitdrinking', 'eatingdisorderrecovery', 'italy', 'chocolatechipcookies', 'school', 'views', '0', 'shopping', 'nyc', 'methadonerecovery', 'kickers', 'saysomethingdosomething', 'substanceabuseawareness', 'respect', 'howto', 'flower', 'lit', 'recoverytiktok', 'interesting', 'medical', 'outfit', '80s', 'morph', 'recoveringaddict', 'nature', 'glowup', 'anxietydisorder', 'pinks', 'treatment', 'o', 'relapsed', 'blacklivesmatter', 'go', 'rojo', 'drums', 'fitcheck', 'ad', 'journey', 'clean', 'foruyou', 'government', 'poem', 'dex', 'candy', 'tweak', 'bowl', 'survivor', 'misinformationkills', 'narcanaccessforall', 'funnyvideo', 'safeaccess', 'aeholidaycard', 'diesel', 'linkagetocare', 'song', 'tips', 'fun', 'addictions', 'griefandloss', 'remember', 'adventure', 'benzos', '2024', 'soul', 'equestrian', 'viralvideos', 'fypシ゚', 'witch', 'herbs', 'thunder', 'christmastrees', 'chocolate', 'opioidusedisorder', 'cleanandserene', 'makethusgoviral', 'tiktoker', 'instadaily', 'draw', 'happygirl', 'performance', 'arthritis', 'ronaldo', 'grow', 'scoop', 'pregnant', 'yes', 'sobercommunity', 'probablygonnapissyouoff', 'partying', 'sublocadeinfo', 'trendingvideo', 'boom', 'oscarsathome', 'psytrance', 'nosecandy', 'opiodawareness', 'snowtok', 'advocate', 'musica', 'holistic', 'facts']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import ast\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/kcore_10_nodes.csv')\n",
    "\n",
    "# Extract the 'node' column and get the count\n",
    "node_list = df['hashtag'].dropna().tolist()\n",
    "node_count = len(node_list)\n",
    "print(f\"Total count of nodes: {node_count}\")\n",
    "\n",
    "# Remove occurrences of all_human_tags from node_list\n",
    "node_list = [node for node in node_list if node not in all_human_tags]\n",
    "\n",
    "# Get the count of remaining nodes\n",
    "node_count = len(node_list)\n",
    "print(f\"Total count of nodes after removal: {node_count}\")\n",
    "\n",
    "# Use all remaining nodes instead of random sampling\n",
    "print(f\"Remaining nodes: {node_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test new function\n",
    "# from get_themes import get_theme\n",
    "# import random\n",
    "# hashtag_sample = random.sample(node_list, 100)\n",
    "# hashtag_sample_str = \", \".join(hashtag_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag_sample_df = pd.DataFrame(hashtag_sample)\n",
    "# hashtag_sample_df.to_csv(\"../data/sample500_batch_4o.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get themes for the sampled hashtags\n",
    "# gpt4o : gpt-4o-2024-05-13\n",
    "# gpt4o-mini: gpt-4o-mini-2024-07-18\n",
    "# themes = get_theme(hashtag_sample_str, model= \"gpt-4o-2024-05-13\", client= client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_themes(themes_string):\n",
    "#     try:\n",
    "#         themes_dict = json.loads(themes_string)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Error decoding JSON: {e}\")\n",
    "#         return pd.DataFrame(columns=['Category', 'Hashtag'])\n",
    "\n",
    "#     # Step 3: Process the dictionary to create a pandas DataFrame\n",
    "#     data = []\n",
    "#     for category, hashtags in themes_dict.items():\n",
    "#         for hashtag in hashtags:\n",
    "#             data.append({'Category': category, 'Hashtag': hashtag})\n",
    "    \n",
    "#     df = pd.DataFrame(data)\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# themes_df = process_themes(themes)\n",
    "# themes_df.to_csv(\"../data/themes100_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment with parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pararallelize import process_posts_in_parallel\n",
    "from get_themes import get_theme\n",
    "import random\n",
    "\n",
    "def chunk_list_to_df(input_list, chunk_size):\n",
    "  \"\"\"\n",
    "  Chunks a list into smaller lists of approximately equal size and returns a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      input_list: The list to be chunked.\n",
    "      chunk_size: The desired size of each chunk.\n",
    "\n",
    "  Returns:\n",
    "      A DataFrame where each row contains a chunk of the original list.\n",
    "  \"\"\"\n",
    "  chunks = []\n",
    "  for i in range(0, len(input_list), chunk_size):\n",
    "    chunks.append(input_list[i:i + chunk_size])\n",
    "\n",
    "  df = pd.DataFrame({'hashtags': chunks})  \n",
    "  df[\"hashtags_str\"] = df[\"hashtags\"].apply(lambda x: ', '.join(x))\n",
    "\n",
    "  return df\n",
    "\n",
    "# hashtag_sample = random.sample(node_list, 100)\n",
    "hashtag_chunks_df = chunk_list_to_df(node_list, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_chunks_df[\"themes\"] = process_posts_in_parallel(hashtag_chunks_df[\"hashtags_str\"].to_list(), max_workers=10,model = \"gpt-4o-2024-05-13\", task = get_theme, client = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_chunks_df.to_csv(\"../data/hashtag_themes_10kcore_2k_20241120.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_dataframe(input_string):\n",
    "    # Parse the JSON string\n",
    "    data = json.loads(input_string)\n",
    "    \n",
    "    # Create lists to store the data\n",
    "    themes = []\n",
    "    hashtags = []\n",
    "    \n",
    "    # Iterate through the dictionary\n",
    "    for theme, tags in data.items():\n",
    "        for tag in tags:\n",
    "            themes.append(theme)\n",
    "            hashtags.append(tag)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'theme': themes,\n",
    "        'hashtag': hashtags\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hashtag_chunks(hashtag_chunks_df):\n",
    "    all_hashtags_themes = pd.DataFrame()\n",
    "    error_rows = []\n",
    "\n",
    "    for index, row in hashtag_chunks_df.iterrows():\n",
    "        try:\n",
    "            theme = row[\"themes\"]\n",
    "            blah = convert_string_to_dataframe(theme)\n",
    "            all_hashtags_themes = pd.concat([blah, all_hashtags_themes], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            error_rows.append({\n",
    "                \"index\": index,\n",
    "                \"theme\": theme,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    return all_hashtags_themes, error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unterminated string starting at: line 7 column 11604 (char 12385)\n",
      "Unterminated string starting at: line 7 column 6339 (char 7148)\n",
      "Unterminated string starting at: line 5 column 9244 (char 9854)\n",
      "Unterminated string starting at: line 9 column 11718 (char 12790)\n",
      "Unterminated string starting at: line 67 column 278 (char 11735)\n"
     ]
    }
   ],
   "source": [
    "result_df, errors = process_hashtag_chunks(hashtag_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_30813/4204586226.py:3: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  errors_df.to_excel(\"../data/hashtag_themes_kcore2k_errors.xlsx\")\n"
     ]
    }
   ],
   "source": [
    "result_df.to_csv(\"../data/hashtag_themes_kcore2k_long.csv\")\n",
    "errors_df = pd.DataFrame(errors)\n",
    "errors_df.to_excel(\"../data/hashtag_themes_kcore2k_errors.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TEST IF NON ASCII CHARACTERS ARE THE PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hashtags: 2333\n",
      "Clean hashtags: 2290\n",
      "Sampled: 500\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_clean_hashtag(text):\n",
    "    \"\"\"Check if hashtag contains only ASCII characters and basic punctuation\"\"\"\n",
    "    return bool(re.match(r'^[a-zA-Z0-9_]+$', str(text)))\n",
    "\n",
    "def sample_clean_hashtags(df, n=500, seed=42):\n",
    "    \"\"\"Sample n clean hashtags from dataframe\"\"\"\n",
    "    # Mark clean hashtags\n",
    "    df['is_clean'] = df['hashtag'].apply(is_clean_hashtag)\n",
    "    \n",
    "    # Sample from clean hashtags\n",
    "    clean_df = df[df['is_clean']].copy()\n",
    "    sample_df = clean_df.sample(n=min(n, len(clean_df)), random_state=seed)\n",
    "    \n",
    "    print(f\"Total hashtags: {len(df)}\")\n",
    "    print(f\"Clean hashtags: {len(clean_df)}\")\n",
    "    print(f\"Sampled: {len(sample_df)}\")\n",
    "    \n",
    "    # Create comma-separated string for GPT\n",
    "    sample_string = ', '.join(sample_df['hashtag'].tolist())\n",
    "    \n",
    "    return sample_df, sample_string\n",
    "\n",
    "# Sample usage\n",
    "sample_df, gpt_input = sample_clean_hashtags(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with GPT\n",
    "response = get_theme(gpt_input, model=\"gpt-4o-2024-05-13\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response:\n",
      "{\n",
      "    \"emotions and feelings\": [\"heartbroken\", \"helpme\", \"happytobealive\", \"vibes\", \"loved\", \"depressedtiktok\", \"laughing\", \"grief\", \"stress\", \"laughter\", \"sadtok\", \"struggle\", \"healingjourney\", \"goodmorning\", \"good\", \"loved\", \"depressionanxiety\", \"depressedtiktok\", \"laughing\", \"grief\", \"stress\", \"laughter\", \"sadtok\", \"struggle\", \"healingjourney\", \"goodmorning\", \"good\"],\n",
      "    \"health conditions\": [\"chronicpain\", \"ptsd\", \"headaches\", \"bipolar\", \"schizophrenic\", \"bipolar1disorder\", \"add\", \"painpatient\", \"chronicpainwarrior\", \"intractablepain\", \"depressionhelp\", \"mentalhealthtok\", \"mentalhealthrecovery\", \"lupus\", \"fibromyalgia\", \"cripple\", \"ms\"],\n",
      "    \"alcohol\": [\"vodka\", \"whiskey\", \"tequila\", \"cocktails\", \"wine\", \"booze\", \"beer\"],\n",
      "    \"cannabis\": [\"weed\", \"stonersoftiktok\", \"dabs\", \"420vibes\", \"auntmary\", \"sinsemilla\", \"grasstok\", \"stoned\", \"puffpuffpass\", \"420\", \"420vibes\", \"grass\"],\n",
      "    \"cognitive enhancement\": [\"modafinil\", \"nootropics\", \"piracetam\", \"cerebrolysin\"],\n",
      "    \"commonly-misused substances\": [\"heroin\", \"xans\", \"percs\", \"lean\", \"fentfriday\", \"blacktar\", \"nosecandy\", \"methadoneisrecovery\", \"benzos\", \"xan\", \"molly\", \"xtc\", \"uppers\", \"plug\", \"crystal\", \"8ball\", \"coughsyrup\", \"pnp\", \"qat\", \"mira\", \"bangi\", \"reds\", \"barbs\", \"blackbeauties\", \"powder\", \"crystaltok\", \"lucy\", \"bhang\", \"opioidsareneccessary\", \"subxone\", \"suboxonesaveslives\", \"methadonerecovery\", \"opioidpainmedicinesaveslives\", \"opioidhysteria\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\", \"opioidawareness\", \"opioiddetox\", \"opioidaddiction\", \"opioid\n",
      "Error: JSON parsing failed\n",
      "\n",
      "First 500 characters of response:\n",
      "{\n",
      "    \"emotions and feelings\": [\"heartbroken\", \"helpme\", \"happytobealive\", \"vibes\", \"loved\", \"depressedtiktok\", \"laughing\", \"grief\", \"stress\", \"laughter\", \"sadtok\", \"struggle\", \"healingjourney\", \"goodmorning\", \"good\", \"loved\", \"depressionanxiety\", \"depressedtiktok\", \"laughing\", \"grief\", \"stress\", \"laughter\", \"sadtok\", \"struggle\", \"healingjourney\", \"goodmorning\", \"good\"],\n",
      "    \"health conditions\": [\"chronicpain\", \"ptsd\", \"headaches\", \"bipolar\", \"schizophrenic\", \"bipolar1disorder\", \"add\", \"painpati\n"
     ]
    }
   ],
   "source": [
    "def validate_gpt_response(response):\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        return parsed, None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Raw response:\")\n",
    "        print(response)\n",
    "        return None, \"JSON parsing failed\"\n",
    "\n",
    "# Test the response\n",
    "parsed, error = validate_gpt_response(response)\n",
    "if parsed:\n",
    "    all_classified = [tag for tags in parsed.values() for tag in tags]\n",
    "    duplicates = pd.Series(all_classified).value_counts()[pd.Series(all_classified).value_counts() > 1]\n",
    "    print(\"\\nDuplicates:\", \"None\" if len(duplicates) == 0 else duplicates)\n",
    "else:\n",
    "    print(\"Error:\", error)\n",
    "\n",
    "# If you want to see the response structure:\n",
    "print(\"\\nFirst 500 characters of response:\")\n",
    "print(response[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems like its donig fine up until this point (attached). i dont know why this behavior is happening. i have a couple of suspicions. maybe its the input length? it cant be non-ascii characters because we just tested that on 500 clean hashtags. maybe its the ask of returning responses in json format?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try data with smaller chunk = 100\n",
    "hashtag_chunks_df = chunk_list_to_df(node_list, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hashtags(hashtag_chunks_df):\n",
    "    # Store responses for each chunk\n",
    "    responses = []\n",
    "    \n",
    "    # Process each chunk and collect responses\n",
    "    for i, chunk in enumerate(hashtag_chunks_df[\"hashtags\"]):\n",
    "        print(f'Processing chunk {i+1}/{len(hashtag_chunks_df[\"hashtags\"])}')\n",
    "        chunk_str = ', '.join(chunk)\n",
    "        response = get_theme(chunk_str, model=\"gpt-4o-2024-05-13\", client=client)\n",
    "        responses.append(response)\n",
    "        print(f\"Response length: {len(response)}\")\n",
    "    \n",
    "    # Parse all responses\n",
    "    all_results = {}\n",
    "    for i, response in enumerate(responses):\n",
    "        try:\n",
    "            parsed = json.loads(response)\n",
    "            print(f\"Successfully parsed chunk {i+1}, found {len(parsed)} categories\")\n",
    "            all_results.update(parsed)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nError parsing chunk {i+1}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            print(f\"First 100 chars of problematic response: {response[:100]}\")\n",
    "            print(f\"Last 100 chars of problematic response: {response[-100:] if len(response) > 100 else response}\")\n",
    "    \n",
    "    print(f\"\\nTotal successful parses: {len(all_results)}\")\n",
    "    return all_results, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/22\n",
      "Error in chunk 1\n",
      "First 100 chars of response: {\n",
      "    \"emotions and feelings\": [\"motivational\", \"strength\", \"depressed\"],\n",
      "    \"health conditions\": [\n",
      "Processing chunk 2/22\n",
      "Processing chunk 3/22\n",
      "Processing chunk 4/22\n",
      "Processing chunk 5/22\n",
      "Processing chunk 6/22\n",
      "Processing chunk 7/22\n",
      "Processing chunk 8/22\n",
      "Processing chunk 9/22\n",
      "Processing chunk 10/22\n",
      "Processing chunk 11/22\n",
      "Error in chunk 11\n",
      "First 100 chars of response: {\n",
      "    \"emotions and feelings\": [\"inspire\", \"anxietyrelief\", \"realtalk\", \"today\"],\n",
      "    \"health condit\n",
      "Processing chunk 12/22\n",
      "Error in chunk 12\n",
      "First 100 chars of response: {\n",
      "    \"emotions and feelings\": [\"missyou\", \"lovely\"],\n",
      "    \"health conditions\": [\"bipolar\", \"addictio\n",
      "Processing chunk 13/22\n",
      "Error in chunk 13\n",
      "First 100 chars of response: {\n",
      "            \"emotions and feelings\": [\"depressedtiktok\", \"loveislove\", \"relax\", \"inspiration\", \"se\n",
      "Processing chunk 14/22\n",
      "Processing chunk 15/22\n",
      "Processing chunk 16/22\n",
      "Processing chunk 17/22\n",
      "Processing chunk 18/22\n",
      "Error in chunk 18\n",
      "First 100 chars of response: {\n",
      "    \"emotions and feelings\": [\"loveyou\", \"grateful\", \"best\", \"relationships\", \"godisgood\", \"gratef\n",
      "Processing chunk 19/22\n",
      "Processing chunk 20/22\n",
      "Processing chunk 21/22\n",
      "Processing chunk 22/22\n"
     ]
    }
   ],
   "source": [
    "results2 = process_hashtags(hashtag_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_themes_simple import get_theme_simple\n",
    "\n",
    "def process_hashtags_batch(hashtags_df, batch_size=10):\n",
    "    \"\"\"Process hashtags in small batches\"\"\"\n",
    "    results = {}\n",
    "    total = len(hashtags_df)\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = hashtags_df['hashtag'].iloc[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(total+batch_size-1)//batch_size}\")\n",
    "        \n",
    "        # Convert batch to comma-separated string\n",
    "        batch_str = ', '.join(batch)\n",
    "        \n",
    "        # Get classifications\n",
    "        response = get_theme_simple(batch_str, model=\"gpt-4o-2024-05-13\", client=client)\n",
    "        \n",
    "        # Parse simple format (hashtag: category)\n",
    "        for line in response.split('\\n'):\n",
    "            if ':' in line:\n",
    "                hashtag, category = line.split(':')\n",
    "                results[hashtag.strip()] = category.strip()\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(\n",
    "        [(k, v) for k, v in results.items()], \n",
    "        columns=['hashtag', 'theme']\n",
    "    )\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/234\n",
      "Processing batch 2/234\n",
      "Processing batch 3/234\n",
      "Processing batch 4/234\n",
      "Processing batch 5/234\n",
      "Processing batch 6/234\n",
      "Processing batch 7/234\n",
      "Processing batch 8/234\n",
      "Processing batch 9/234\n",
      "Processing batch 10/234\n",
      "Processing batch 11/234\n",
      "Processing batch 12/234\n",
      "Processing batch 13/234\n",
      "Processing batch 14/234\n",
      "Processing batch 15/234\n",
      "Processing batch 16/234\n",
      "Processing batch 17/234\n",
      "Processing batch 18/234\n",
      "Processing batch 19/234\n",
      "Processing batch 20/234\n",
      "Processing batch 21/234\n",
      "Processing batch 22/234\n",
      "Processing batch 23/234\n",
      "Processing batch 24/234\n",
      "Processing batch 25/234\n",
      "Processing batch 26/234\n",
      "Processing batch 27/234\n",
      "Processing batch 28/234\n",
      "Processing batch 29/234\n",
      "Processing batch 30/234\n",
      "Processing batch 31/234\n",
      "Processing batch 32/234\n",
      "Processing batch 33/234\n",
      "Processing batch 34/234\n",
      "Processing batch 35/234\n",
      "Processing batch 36/234\n",
      "Processing batch 37/234\n",
      "Processing batch 38/234\n",
      "Processing batch 39/234\n",
      "Processing batch 40/234\n",
      "Processing batch 41/234\n",
      "Processing batch 42/234\n",
      "Processing batch 43/234\n",
      "Processing batch 44/234\n",
      "Processing batch 45/234\n",
      "Processing batch 46/234\n",
      "Processing batch 47/234\n",
      "Processing batch 48/234\n",
      "Processing batch 49/234\n",
      "Processing batch 50/234\n",
      "Processing batch 51/234\n",
      "Processing batch 52/234\n",
      "Processing batch 53/234\n",
      "Processing batch 54/234\n",
      "Processing batch 55/234\n",
      "Processing batch 56/234\n",
      "Processing batch 57/234\n",
      "Processing batch 58/234\n",
      "Processing batch 59/234\n",
      "Processing batch 60/234\n",
      "Processing batch 61/234\n",
      "Processing batch 62/234\n",
      "Processing batch 63/234\n",
      "Processing batch 64/234\n",
      "Processing batch 65/234\n",
      "Processing batch 66/234\n",
      "Processing batch 67/234\n",
      "Processing batch 68/234\n",
      "Processing batch 69/234\n",
      "Processing batch 70/234\n",
      "Processing batch 71/234\n",
      "Processing batch 72/234\n",
      "Processing batch 73/234\n",
      "Processing batch 74/234\n",
      "Processing batch 75/234\n",
      "Processing batch 76/234\n",
      "Processing batch 77/234\n",
      "Processing batch 78/234\n",
      "Processing batch 79/234\n",
      "Processing batch 80/234\n",
      "Processing batch 81/234\n",
      "Processing batch 82/234\n",
      "Processing batch 83/234\n",
      "Processing batch 84/234\n",
      "Processing batch 85/234\n",
      "Processing batch 86/234\n",
      "Processing batch 87/234\n",
      "Processing batch 88/234\n",
      "Processing batch 89/234\n",
      "Processing batch 90/234\n",
      "Processing batch 91/234\n",
      "Processing batch 92/234\n",
      "Processing batch 93/234\n",
      "Processing batch 94/234\n",
      "Processing batch 95/234\n",
      "Processing batch 96/234\n",
      "Processing batch 97/234\n",
      "Processing batch 98/234\n",
      "Processing batch 99/234\n",
      "Processing batch 100/234\n",
      "Processing batch 101/234\n",
      "Processing batch 102/234\n",
      "Processing batch 103/234\n",
      "Processing batch 104/234\n",
      "Processing batch 105/234\n",
      "Processing batch 106/234\n",
      "Processing batch 107/234\n",
      "Processing batch 108/234\n",
      "Processing batch 109/234\n",
      "Processing batch 110/234\n",
      "Processing batch 111/234\n",
      "Processing batch 112/234\n",
      "Processing batch 113/234\n",
      "Processing batch 114/234\n",
      "Processing batch 115/234\n",
      "Processing batch 116/234\n",
      "Processing batch 117/234\n",
      "Processing batch 118/234\n",
      "Processing batch 119/234\n",
      "Processing batch 120/234\n",
      "Processing batch 121/234\n",
      "Processing batch 122/234\n",
      "Processing batch 123/234\n",
      "Processing batch 124/234\n",
      "Processing batch 125/234\n",
      "Processing batch 126/234\n",
      "Processing batch 127/234\n",
      "Processing batch 128/234\n",
      "Processing batch 129/234\n",
      "Processing batch 130/234\n",
      "Processing batch 131/234\n",
      "Processing batch 132/234\n",
      "Processing batch 133/234\n",
      "Processing batch 134/234\n",
      "Processing batch 135/234\n",
      "Processing batch 136/234\n",
      "Processing batch 137/234\n",
      "Processing batch 138/234\n",
      "Processing batch 139/234\n",
      "Processing batch 140/234\n",
      "Processing batch 141/234\n",
      "Processing batch 142/234\n",
      "Processing batch 143/234\n",
      "Processing batch 144/234\n",
      "Processing batch 145/234\n",
      "Processing batch 146/234\n",
      "Processing batch 147/234\n",
      "Processing batch 148/234\n",
      "Processing batch 149/234\n",
      "Processing batch 150/234\n",
      "Processing batch 151/234\n",
      "Processing batch 152/234\n",
      "Processing batch 153/234\n",
      "Processing batch 154/234\n",
      "Processing batch 155/234\n",
      "Processing batch 156/234\n",
      "Processing batch 157/234\n",
      "Processing batch 158/234\n",
      "Processing batch 159/234\n",
      "Processing batch 160/234\n",
      "Processing batch 161/234\n",
      "Processing batch 162/234\n",
      "Processing batch 163/234\n",
      "Processing batch 164/234\n",
      "Processing batch 165/234\n",
      "Processing batch 166/234\n",
      "Processing batch 167/234\n",
      "Processing batch 168/234\n",
      "Processing batch 169/234\n",
      "Processing batch 170/234\n",
      "Processing batch 171/234\n",
      "Processing batch 172/234\n",
      "Processing batch 173/234\n",
      "Processing batch 174/234\n",
      "Processing batch 175/234\n",
      "Processing batch 176/234\n",
      "Processing batch 177/234\n",
      "Processing batch 178/234\n",
      "Processing batch 179/234\n",
      "Processing batch 180/234\n",
      "Processing batch 181/234\n",
      "Processing batch 182/234\n",
      "Processing batch 183/234\n",
      "Processing batch 184/234\n",
      "Processing batch 185/234\n",
      "Processing batch 186/234\n",
      "Processing batch 187/234\n",
      "Processing batch 188/234\n",
      "Processing batch 189/234\n",
      "Processing batch 190/234\n",
      "Processing batch 191/234\n",
      "Processing batch 192/234\n",
      "Processing batch 193/234\n",
      "Processing batch 194/234\n",
      "Processing batch 195/234\n",
      "Processing batch 196/234\n",
      "Processing batch 197/234\n",
      "Processing batch 198/234\n",
      "Processing batch 199/234\n",
      "Processing batch 200/234\n",
      "Processing batch 201/234\n",
      "Processing batch 202/234\n",
      "Processing batch 203/234\n",
      "Processing batch 204/234\n",
      "Processing batch 205/234\n",
      "Processing batch 206/234\n",
      "Processing batch 207/234\n",
      "Processing batch 208/234\n",
      "Processing batch 209/234\n",
      "Processing batch 210/234\n",
      "Processing batch 211/234\n",
      "Processing batch 212/234\n",
      "Processing batch 213/234\n",
      "Processing batch 214/234\n",
      "Processing batch 215/234\n",
      "Processing batch 216/234\n",
      "Processing batch 217/234\n",
      "Processing batch 218/234\n",
      "Processing batch 219/234\n",
      "Processing batch 220/234\n",
      "Processing batch 221/234\n",
      "Processing batch 222/234\n",
      "Processing batch 223/234\n",
      "Processing batch 224/234\n",
      "Processing batch 225/234\n",
      "Processing batch 226/234\n",
      "Processing batch 227/234\n",
      "Processing batch 228/234\n",
      "Processing batch 229/234\n",
      "Processing batch 230/234\n",
      "Processing batch 231/234\n",
      "Processing batch 232/234\n",
      "Processing batch 233/234\n",
      "Processing batch 234/234\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10  # You can adjust this number\n",
    "results_df3 = process_hashtags_batch(df, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df3.to_csv(\"../data/themes_10core_20241120.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After labeling hashtags one at at time, we address the issue of missing hashtags so the subsequent code may be uneccesary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unlabeled_hashtags(original_df: pd.DataFrame, labeled_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Assuming the hashtag column is named the same in both dataframes\n",
    "    unlabeled = original_df[~original_df['hashtag'].isin(labeled_df['hashtag'])]\n",
    "    print(f\"Original hashtags: {len(original_df)}\")\n",
    "    print(f\"Labeled hashtags: {len(labeled_df)}\")\n",
    "    print(f\"Unlabeled hashtags: {len(unlabeled)}\")\n",
    "    \n",
    "    return unlabeled\n",
    "\n",
    "def prepare_unlabeled_for_gpt(unlabeled_df: pd.DataFrame) -> str:\n",
    "    return ', '.join(unlabeled_df['hashtag'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hashtags: 5519\n",
      "Labeled hashtags: 5242\n",
      "Unlabeled hashtags: 408\n"
     ]
    }
   ],
   "source": [
    "unlabeled_df = find_unlabeled_hashtags(df, result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags with multiple themes:\n",
      "                  theme     hashtag\n",
      "2372           platform         911\n",
      "2421               misc         911\n",
      "2434               misc         911\n",
      "544   substance effects  blackedout\n",
      "2375  substance effects  blackedout\n",
      "...                 ...         ...\n",
      "476            platform     تابعوني\n",
      "5045               misc         ضحك\n",
      "2916               misc         ضحك\n",
      "2429               misc       ❄️tok\n",
      "2367           platform       ❄️tok\n",
      "\n",
      "[181 rows x 2 columns]\n",
      "\n",
      "Summary of multiple labels:\n",
      "hashtag\n",
      "911                                      [platform, misc, misc]\n",
      "blackedout               [substance effects, substance effects]\n",
      "boston                                     [location, location]\n",
      "brand                                    [platform, misc, misc]\n",
      "cigs                       [tobacco_nicotine, tobacco_nicotine]\n",
      "                                        ...                    \n",
      "xylazineawareness    [awareness and advocacy, other substances]\n",
      "xylazinetesting      [awareness and advocacy, other substances]\n",
      "تابعوني                                        [platform, misc]\n",
      "ضحك                                                [misc, misc]\n",
      "❄️tok                                          [platform, misc]\n",
      "Name: theme, Length: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "duplicated_hashtags = result_df[result_df.duplicated(subset=['hashtag'], keep=False)]\n",
    "\n",
    "# Sort by hashtag to see the groupings easily\n",
    "print(\"Hashtags with multiple themes:\")\n",
    "print(duplicated_hashtags.sort_values('hashtag'))\n",
    "\n",
    "# To see a cleaner summary:\n",
    "summary = duplicated_hashtags.groupby('hashtag')['theme'].agg(list)\n",
    "print(\"\\nSummary of multiple labels:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/bt0ykz9n5_16z2gvbybl0tbc0000gn/T/ipykernel_21220/3331842704.py:2: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  summary.to_excel(\"../data/duplicated_69_hashtags.xlsx\")\n"
     ]
    }
   ],
   "source": [
    "# manually select the correct theme from the 25 hashtags that were duplicated\n",
    "summary.to_excel(\"../data/duplicated_69_hashtags.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of results_df after removing duplicates: (5061, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove all instances of duplicated hashtags\n",
    "result_df = result_df[~result_df['hashtag'].isin(duplicated_hashtags['hashtag'])]\n",
    "print(f\"Shape of results_df after removing duplicates: {result_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixes_df = pd.read_excel(\"../data/duplicated_25_hashtags.xlsx\")\n",
    "fixes_df['theme'] = fixes_df['theme'].str.strip('[]').str.strip(\"'\")  # Remove [] and quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([result_df, fixes_df], ignore_index=True)\n",
    "result_df = result_df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Still have duplicated hashtags:\n",
      "                       theme                     hashtag\n",
      "160        health conditions         depressionawareness\n",
      "5061  awareness and advocacy         depressionawareness\n",
      "5062  awareness and advocacy      drugpoisoningawareness\n",
      "3675       health conditions      drugpoisoningawareness\n",
      "5063  awareness and advocacy  fentanylpoisoningawareness\n",
      "4254       health conditions  fentanylpoisoningawareness\n",
      "1390                   humor               funnypictures\n",
      "5065                    misc               funnypictures\n",
      "1947  awareness and advocacy     harmreductionspecialist\n",
      "5068              occupation     harmreductionspecialist\n",
      "3677       health conditions                hivawareness\n",
      "5070  awareness and advocacy                hivawareness\n",
      "2566                   humor                        ohno\n",
      "5075                    misc                        ohno\n",
      "453        health conditions          psychosisawareness\n",
      "5078  awareness and advocacy          psychosisawareness\n",
      "4021                cannabis                       rasta\n",
      "5079  identity and community                       rasta\n",
      "2760                   humor                  yerawizard\n",
      "5084                    misc                  yerawizard\n"
     ]
    }
   ],
   "source": [
    "# Verify no more duplicates\n",
    "new_duplicates = result_df[result_df.duplicated(subset=['hashtag'], keep=False)]\n",
    "if not new_duplicates.empty:\n",
    "    print(\"\\nStill have duplicated hashtags:\")\n",
    "    print(new_duplicates.sort_values('hashtag'))\n",
    "else:\n",
    "    print(\"No duplicates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label just the missing 776\n",
    "missing_hashtag_chunks_df = chunk_list_to_df(unlabeled_df.hashtag, 50)\n",
    "missing_hashtag_chunks_df[\"themes\"] = process_posts_in_parallel(missing_hashtag_chunks_df[\"hashtags_str\"].to_list(), max_workers=10,model = \"gpt-4o-2024-05-13\", task = get_theme, client = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_result_df, missing_errors = process_hashtag_chunks(missing_hashtag_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtags with multiple themes:\n",
      "                      theme           hashtag\n",
      "323        tobacco_nicotine               cig\n",
      "306      consumption method               cig\n",
      "280        other substances          creatine\n",
      "375        other substances          creatine\n",
      "221      consumption method              ecig\n",
      "242        tobacco_nicotine              ecig\n",
      "328              occupation  frontlineworkers\n",
      "407              occupation  frontlineworkers\n",
      "291              occupation  frontlineworkers\n",
      "322       substance effects        highasfuck\n",
      "391       substance effects        highasfuck\n",
      "181        tobacco_nicotine          nicotine\n",
      "393        tobacco_nicotine          nicotine\n",
      "376        other substances         piracetam\n",
      "279        other substances         piracetam\n",
      "401                   humor        sobermemes\n",
      "325                   humor        sobermemes\n",
      "289                   humor        sobermemes\n",
      "182        tobacco_nicotine       stopsmoking\n",
      "164      consumption method       stopsmoking\n",
      "329  identity and community          transtok\n",
      "292  identity and community          transtok\n",
      "412  identity and community          transtok\n",
      "290                location               usa\n",
      "402                location               usa\n",
      "327                location               usa\n",
      "287        tobacco_nicotine              vape\n",
      "271      consumption method              vape\n",
      "288        tobacco_nicotine             vaper\n",
      "274      consumption method             vaper\n",
      "\n",
      "Summary of multiple labels:\n",
      "hashtag\n",
      "cig                            [consumption method, tobacco_nicotine]\n",
      "creatine                         [other substances, other substances]\n",
      "ecig                           [consumption method, tobacco_nicotine]\n",
      "frontlineworkers                 [occupation, occupation, occupation]\n",
      "highasfuck                     [substance effects, substance effects]\n",
      "nicotine                         [tobacco_nicotine, tobacco_nicotine]\n",
      "piracetam                        [other substances, other substances]\n",
      "sobermemes                                      [humor, humor, humor]\n",
      "stopsmoking                    [consumption method, tobacco_nicotine]\n",
      "transtok            [identity and community, identity and communit...\n",
      "usa                                    [location, location, location]\n",
      "vape                           [consumption method, tobacco_nicotine]\n",
      "vaper                          [consumption method, tobacco_nicotine]\n",
      "Name: theme, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# see if there are dupes in the second round\n",
    "duplicated_hashtags2 = missing_result_df[missing_result_df.duplicated(subset=['hashtag'], keep=False)]\n",
    "\n",
    "# Sort by hashtag to see the groupings easily\n",
    "print(\"Hashtags with multiple themes:\")\n",
    "print(duplicated_hashtags2.sort_values('hashtag'))\n",
    "\n",
    "# To see a cleaner summary:\n",
    "summary2 = duplicated_hashtags2.groupby('hashtag')['theme'].agg(list)\n",
    "print(\"\\nSummary of multiple labels:\")\n",
    "print(summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hashtags: 408\n",
      "Labeled hashtags: 430\n",
      "Unlabeled hashtags: 41\n"
     ]
    }
   ],
   "source": [
    "# GET THE STILL MISSING SET FROM THE SECOND BATCH\n",
    "unlabeled_df2 = find_unlabeled_hashtags(unlabeled_df, missing_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label just the missing 31\n",
    "missing_hashtag_chunks_df2 = chunk_list_to_df(unlabeled_df2.hashtag, 50)\n",
    "missing_hashtag_chunks_df2[\"themes\"] = process_posts_in_parallel(missing_hashtag_chunks_df2[\"hashtags_str\"].to_list(), max_workers=10,model = \"gpt-4o-2024-05-13\", task = get_theme, client = client)\n",
    "missing_result_df2, missing_errors2 = process_hashtag_chunks(missing_hashtag_chunks_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append\n",
    "result_full = pd.concat([result_df, missing_result_df, missing_result_df2])\n",
    "result_full = result_full.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Still have duplicated hashtags:\n",
      "                       theme                     hashtag\n",
      "1046                platform             @octopusdarling\n",
      "104                     misc             @octopusdarling\n",
      "323         tobacco_nicotine                         cig\n",
      "306       consumption method                         cig\n",
      "160        health conditions         depressionawareness\n",
      "5061  awareness and advocacy         depressionawareness\n",
      "3675       health conditions      drugpoisoningawareness\n",
      "5062  awareness and advocacy      drugpoisoningawareness\n",
      "242         tobacco_nicotine                        ecig\n",
      "221       consumption method                        ecig\n",
      "4254       health conditions  fentanylpoisoningawareness\n",
      "5063  awareness and advocacy  fentanylpoisoningawareness\n",
      "1390                   humor               funnypictures\n",
      "5065                    misc               funnypictures\n",
      "1947  awareness and advocacy     harmreductionspecialist\n",
      "5068              occupation     harmreductionspecialist\n",
      "5070  awareness and advocacy                hivawareness\n",
      "3677       health conditions                hivawareness\n",
      "5075                    misc                        ohno\n",
      "2566                   humor                        ohno\n",
      "5078  awareness and advocacy          psychosisawareness\n",
      "453        health conditions          psychosisawareness\n",
      "5079  identity and community                       rasta\n",
      "4021                cannabis                       rasta\n",
      "14                  platform       socialworkersoftiktok\n",
      "22                occupation       socialworkersoftiktok\n",
      "164       consumption method                 stopsmoking\n",
      "182         tobacco_nicotine                 stopsmoking\n",
      "271       consumption method                        vape\n",
      "287         tobacco_nicotine                        vape\n",
      "274       consumption method                       vaper\n",
      "288         tobacco_nicotine                       vaper\n",
      "5084                    misc                  yerawizard\n",
      "2760                   humor                  yerawizard\n"
     ]
    }
   ],
   "source": [
    "new_duplicates = result_full[result_full.duplicated(subset=['hashtag'], keep=False)]\n",
    "if not new_duplicates.empty:\n",
    "    print(\"\\nStill have duplicated hashtags:\")\n",
    "    print(new_duplicates.sort_values('hashtag'))\n",
    "else:\n",
    "    print(\"No duplicates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current entries for @octopusdarling:\n",
      "         theme          hashtag\n",
      "1046  platform  @octopusdarling\n",
      "104       misc  @octopusdarling\n",
      "\n",
      "After fix:\n",
      "     theme          hashtag\n",
      "5522  misc  @octopusdarling\n",
      "\n",
      "Checking for any remaining duplicates:\n",
      "                       theme                     hashtag\n",
      "160        health conditions         depressionawareness\n",
      "453        health conditions          psychosisawareness\n",
      "1389                   humor               funnypictures\n",
      "1946  awareness and advocacy     harmreductionspecialist\n",
      "2565                   humor                        ohno\n",
      "2759                   humor                  yerawizard\n",
      "3674       health conditions      drugpoisoningawareness\n",
      "3676       health conditions                hivawareness\n",
      "4020                cannabis                       rasta\n",
      "4253       health conditions  fentanylpoisoningawareness\n",
      "5060  awareness and advocacy         depressionawareness\n",
      "5061  awareness and advocacy      drugpoisoningawareness\n",
      "5062  awareness and advocacy  fentanylpoisoningawareness\n",
      "5064                    misc               funnypictures\n",
      "5065              occupation     harmreductionspecialist\n",
      "5067  awareness and advocacy                hivawareness\n",
      "5069                    misc                        ohno\n",
      "5071  awareness and advocacy          psychosisawareness\n",
      "5072  identity and community                       rasta\n",
      "5075                    misc                  yerawizard\n",
      "5240      consumption method                 stopsmoking\n",
      "5258        tobacco_nicotine                 stopsmoking\n",
      "5296      consumption method                        ecig\n",
      "5316        tobacco_nicotine                        ecig\n",
      "5345      consumption method                        vape\n",
      "5348      consumption method                       vaper\n",
      "5360        tobacco_nicotine                        vape\n",
      "5361        tobacco_nicotine                       vaper\n",
      "5379      consumption method                         cig\n",
      "5395        tobacco_nicotine                         cig\n",
      "5500                platform       socialworkersoftiktok\n",
      "5508              occupation       socialworkersoftiktok\n"
     ]
    }
   ],
   "source": [
    "print(\"Current entries for @octopusdarling:\")\n",
    "print(result_full[result_full['hashtag'] == '@octopusdarling'])\n",
    "\n",
    "# Assuming we want to keep 'platform' theme and remove 'misc'\n",
    "# Remove both instances\n",
    "result_full = result_full[result_full['hashtag'] != '@octopusdarling']\n",
    "\n",
    "# Add back the correct one\n",
    "new_row = pd.DataFrame({\n",
    "    'hashtag': ['@octopusdarling'],\n",
    "    'theme': ['misc'] \n",
    "})\n",
    "\n",
    "# Add the correct row back\n",
    "result_full = pd.concat([result_full, new_row], ignore_index=True)\n",
    "\n",
    "# Verify the fix\n",
    "print(\"\\nAfter fix:\")\n",
    "print(result_full[result_full['hashtag'] == '@octopusdarling'])\n",
    "\n",
    "# Double check no more duplicates\n",
    "duplicates = result_full[result_full.duplicated(subset=['hashtag'], keep=False)]\n",
    "print(\"\\nChecking for any remaining duplicates:\")\n",
    "print(duplicates if not duplicates.empty else \"No duplicates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hashtags: 5523\n",
      "Labeled hashtags: 5519\n",
      "Unlabeled hashtags: 22\n"
     ]
    }
   ],
   "source": [
    "unlabeled_df3 = find_unlabeled_hashtags(result_full, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label just the missing 11\n",
    "missing_hashtag_chunks_df3 = chunk_list_to_df(unlabeled_df3.hashtag, 50)\n",
    "missing_hashtag_chunks_df3[\"themes\"] = process_posts_in_parallel(missing_hashtag_chunks_df3[\"hashtags_str\"].to_list(), max_workers=10,model = \"gpt-4o-2024-05-13\", task = get_theme, client = client)\n",
    "missing_result_df3, missing_errors3 = process_hashtag_chunks(missing_hashtag_chunks_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_full2 = pd.concat([result_full, missing_result_df3])\n",
    "# result_full2 = result_full2.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "result_full2.to_csv(\"../data/hashtags_5core_with_themes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muqi got the errored chunks to parse. gonna make them long\n",
    "missing = pd.read_csv(\"../data/themes_missing_parallel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hashtag_chunks_df.themes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(missing.themes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_long, missing_long_e = process_hashtag_chunks(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13758\n"
     ]
    }
   ],
   "source": [
    "# append to long\n",
    "results_full = pd.concat([result_df,missing_long])\n",
    "print(len(set(result_df.hashtag)))\n",
    "# drop dupes\n",
    "results_full_unique = results_full.drop_duplicates(subset=\"hashtag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full_unique.to_csv(\"../data/themes_with_missing_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experient with 500 posts\n",
    "# hashtag_sample2 = random.sample(node_list, 500)\n",
    "# hashtag_chunks_df2 = chunk_list_to_df(hashtag_sample2, 50)\n",
    "# hashtag_chunks_df2[\"themes\"] = process_posts_in_parallel(hashtag_chunks_df2[\"hashtags_str\"].to_list(), max_workers=10,model = \"gpt-4o-2024-05-13\", task = get_theme, client = client)\n",
    "# hashtag_chunks_df2.to_csv(\"../data/themes_500_parallel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment with chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_themes import get_theme\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def chunk_list(input_list, chunk_size):\n",
    "  \"\"\"\n",
    "  Chunks a list into smaller lists of approximately equal size.\n",
    "\n",
    "  Args:\n",
    "      input_list: The list to be chunked.\n",
    "      chunk_size: The desired size of each chunk.\n",
    "\n",
    "  Returns:\n",
    "      A list of lists, where each inner list is a chunk of the original list.\n",
    "  \"\"\"\n",
    "  chunks = []\n",
    "  for i in range(0, len(input_list), chunk_size):\n",
    "    chunks.append(input_list[i:i + chunk_size])\n",
    "  return chunks\n",
    "\n",
    "def chunk_and_analyze_hashtags(hashtag_list, chunks, model, client, chunk_size=50, output_file='../data/hashtags_themes.csv'):\n",
    "    # Shuffle the list to ensure random sampling\n",
    "    random.shuffle(hashtag_list)\n",
    "    \n",
    "    # Chunk the list into batches \n",
    "    # chunks = [hashtag_list[i:i + chunk_size] for i in range(0, len(hashtag_list), chunk_size)]\n",
    "    \n",
    "    total_hashtags = 0\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Theme', 'Hashtag'])  # Write header\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Convert chunk to comma-separated string\n",
    "            print(\"length of chunk: \",len(chunk))\n",
    "            chunk_str = \", \".join(chunk)\n",
    "            \n",
    "            # Get themes for this chunk\n",
    "            themes_str = get_theme(chunk_str, model=model, client=client)\n",
    "            print(themes_str)\n",
    "            themes = json.loads(themes_str.replace(\"'\", '\"'))\n",
    "            # Write results to CSV and count hashtags\n",
    "            chunk_hashtag_count = 0\n",
    "            for theme, hashtags in themes.items():\n",
    "                for hashtag in hashtags:\n",
    "                    csvwriter.writerow([theme, hashtag])\n",
    "                    chunk_hashtag_count += 1\n",
    "            \n",
    "            total_hashtags += chunk_hashtag_count\n",
    "            print(f\"Processed chunk {i+1}/{len(chunks)}. Hashtags in this chunk: {chunk_hashtag_count}\")\n",
    "    \n",
    "    print(f\"Total hashtags processed: {total_hashtags}\")\n",
    "    print(f\"Results have been written to {output_file}\")\n",
    "    return total_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "hashtag_sample = random.sample(node_list, 100)\n",
    "chunks = chunk_list(hashtag_sample, 50)\n",
    "results = chunk_and_analyze_hashtags(hashtag_sample, model=\"gpt-4o-2024-05-13\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, themes in enumerate(results):\n",
    "#     print(f\"Themes for chunk {i+1}:\")\n",
    "#     print(themes)\n",
    "#     print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
